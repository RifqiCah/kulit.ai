{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd0ed314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials in environment variables.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Bucket: kulit-ai-dataset-real-fiki-2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2025-12-30-18-09-16-410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-30 18:09:24 Starting - Starting the training job\n",
      "2025-12-30 18:09:24 Pending - Training job waiting for capacity...\n",
      "2025-12-30 18:09:59 Pending - Preparing the instances for training...\n",
      "2025-12-30 18:10:24 Downloading - Downloading input data...\n",
      "2025-12-30 18:10:59 Downloading - Downloading the training image..............bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "/opt/conda/lib/python3.9/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\n",
      "/opt/conda/lib/python3.9/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\n",
      "2025-12-30 18:13:49,624 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2025-12-30 18:13:49,647 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-12-30 18:13:49,661 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2025-12-30 18:13:49,665 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2025-12-30 18:13:50,956 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-12-30 18:13:51,006 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-12-30 18:13:51,055 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2025-12-30 18:13:51,075 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 8,\n",
      "        \"epochs\": 100,\n",
      "        \"patience\": 5\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"pytorch-training-2025-12-30-18-09-16-410\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-564415061686/pytorch-training-2025-12-30-18-09-16-410/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"sagemaker_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"sagemaker_train.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"batch-size\":8,\"epochs\":100,\"patience\":5}\n",
      "SM_USER_ENTRY_POINT=sagemaker_train.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\n",
      "SM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"test\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.g4dn.xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=sagemaker_train\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=4\n",
      "SM_NUM_GPUS=1\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-east-1-564415061686/pytorch-training-2025-12-30-18-09-16-410/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g4dn.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":8,\"epochs\":100,\"patience\":5},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"is_smddprun_installed\":true,\"job_name\":\"pytorch-training-2025-12-30-18-09-16-410\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-564415061686/pytorch-training-2025-12-30-18-09-16-410/source/sourcedir.tar.gz\",\"module_name\":\"sagemaker_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"sagemaker_train.py\"}\n",
      "SM_USER_ARGS=[\"--batch-size\",\"8\",\"--epochs\",\"100\",\"--patience\",\"5\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_BATCH-SIZE=8\n",
      "SM_HP_EPOCHS=100\n",
      "SM_HP_PATIENCE=5\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.9 sagemaker_train.py --batch-size 8 --epochs 100 --patience 5\n",
      "2025-12-30 18:13:51,117 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "üöÄ Starting Training EfficientNetV2-L\n",
      "‚öôÔ∏è Using device: cuda\n",
      "‚úÖ Classes: ['dry', 'normal', 'oily']\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_v2_l-59c71312.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_v2_l-59c71312.pth\n",
      "0%|          | 0.00/455M [00:00<?, ?B/s]\n",
      "9%|‚ñâ         | 41.4M/455M [00:00<00:00, 434MB/s]\n",
      "19%|‚ñà‚ñä        | 84.7M/455M [00:00<00:00, 446MB/s]\n",
      "28%|‚ñà‚ñà‚ñä       | 129M/455M [00:00<00:00, 456MB/s]\n",
      "38%|‚ñà‚ñà‚ñà‚ñä      | 174M/455M [00:00<00:00, 459MB/s]\n",
      "48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 219M/455M [00:00<00:00, 463MB/s]\n",
      "58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 263M/455M [00:00<00:00, 465MB/s]\n",
      "68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 307M/455M [00:00<00:00, 465MB/s]\n",
      "77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 352M/455M [00:00<00:00, 455MB/s]\n",
      "87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 395M/455M [00:00<00:00, 421MB/s]\n",
      "96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 436M/455M [00:01<00:00, 405MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 455M/455M [00:01<00:00, 434MB/s]\n",
      "\n",
      "2025-12-30 18:13:41 Training - Training image download completed. Training in progress.[2025-12-30 18:13:58.433 algo-1:65 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2025-12-30 18:13:58.918 algo-1:65 INFO profiler_config_parser.py:111] User has disabled profiler.\n",
      "[2025-12-30 18:13:58.918 algo-1:65 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2025-12-30 18:13:58.919 algo-1:65 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2025-12-30 18:13:58.919 algo-1:65 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2025-12-30 18:13:58.919 algo-1:65 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "Epoch [1/100] Train Loss: 1.1318 | Val Loss: 1.1130\n",
      "‚úÖ Model membaik (best updated)\n",
      "Epoch [2/100] Train Loss: 1.1295 | Val Loss: 1.1429\n",
      "‚ö†Ô∏è No improvement (1/5)\n",
      "Epoch [3/100] Train Loss: 1.1127 | Val Loss: 1.1208\n",
      "‚ö†Ô∏è No improvement (2/5)\n",
      "Epoch [4/100] Train Loss: 1.1085 | Val Loss: 1.1048\n",
      "‚úÖ Model membaik (best updated)\n",
      "Epoch [5/100] Train Loss: 1.1106 | Val Loss: 1.1506\n",
      "‚ö†Ô∏è No improvement (1/5)\n",
      "Epoch [6/100] Train Loss: 1.1001 | Val Loss: 1.1066\n",
      "‚ö†Ô∏è No improvement (2/5)\n",
      "Epoch [7/100] Train Loss: 1.0957 | Val Loss: 1.1036\n",
      "‚úÖ Model membaik (best updated)\n",
      "Epoch [8/100] Train Loss: 1.0816 | Val Loss: 1.0981\n",
      "‚úÖ Model membaik (best updated)\n",
      "Epoch [9/100] Train Loss: 1.0987 | Val Loss: 1.1038\n",
      "‚ö†Ô∏è No improvement (1/5)\n",
      "Epoch [10/100] Train Loss: 1.0903 | Val Loss: 1.0995\n",
      "‚ö†Ô∏è No improvement (2/5)\n",
      "Epoch [11/100] Train Loss: 1.0903 | Val Loss: 1.0929\n",
      "‚úÖ Model membaik (best updated)\n",
      "Epoch [12/100] Train Loss: 1.0881 | Val Loss: 1.0945\n",
      "‚ö†Ô∏è No improvement (1/5)\n",
      "Epoch [13/100] Train Loss: 1.0767 | Val Loss: 1.1658\n",
      "‚ö†Ô∏è No improvement (2/5)\n",
      "Epoch [14/100] Train Loss: 1.0785 | Val Loss: 1.0936\n",
      "‚ö†Ô∏è No improvement (3/5)\n",
      "Epoch [15/100] Train Loss: 1.0736 | Val Loss: 1.0877\n",
      "‚úÖ Model membaik (best updated)\n",
      "Epoch [16/100] Train Loss: 1.0797 | Val Loss: 1.0876\n",
      "‚úÖ Model membaik (best updated)\n",
      "Epoch [17/100] Train Loss: 1.0706 | Val Loss: 1.0974\n",
      "‚ö†Ô∏è No improvement (1/5)\n",
      "Epoch [18/100] Train Loss: 1.0689 | Val Loss: 1.1566\n",
      "‚ö†Ô∏è No improvement (2/5)\n",
      "Epoch [19/100] Train Loss: 1.0690 | Val Loss: 1.0905\n",
      "‚ö†Ô∏è No improvement (3/5)\n",
      "Epoch [20/100] Train Loss: 1.0740 | Val Loss: 1.0964\n",
      "‚ö†Ô∏è No improvement (4/5)\n",
      "Epoch [21/100] Train Loss: 1.0582 | Val Loss: 1.1086\n",
      "‚ö†Ô∏è No improvement (5/5)\n",
      "üõë Early Stopping Triggered\n",
      "üìä CLASSIFICATION REPORT (BEST / LAST EPOCH)\n",
      "üèÜ Best Epoch: 16\n",
      "precision    recall  f1-score   support\n",
      "         dry       0.43      0.04      0.08        71\n",
      "      normal       0.42      0.67      0.52       111\n",
      "        oily       0.32      0.31      0.31        80\n",
      "    accuracy                           0.39       262\n",
      "   macro avg       0.39      0.34      0.30       262\n",
      "weighted avg       0.39      0.39      0.34       262\n",
      "üíæ Model disimpan di: /opt/ml/model/model.pth\n",
      "‚úÖ Training selesai\n",
      "2025-12-30 18:54:03,264 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2025-12-30 18:54:03,264 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2025-12-30 18:54:03,265 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2025-12-30 18:54:04 Uploading - Uploading generated training model\n",
      "2025-12-30 18:54:27 Completed - Training job completed\n",
      "Training seconds: 2643\n",
      "Billable seconds: 2643\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.session import Session\n",
    "import boto3\n",
    "\n",
    "boto_sess = boto3.Session(region_name=\"us-east-1\")\n",
    "sm_sess = Session(boto_session=boto_sess)\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket_name = \"kulit-ai-dataset-real-fiki-2025\" \n",
    "role = \"arn:aws:iam::564415061686:role/service-role/AmazonSageMakerAdminIAMExecutionRole\"\n",
    "\n",
    "print(f\"Target Bucket: {bucket_name}\")\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point='sagemaker_train.py',  \n",
    "    source_dir='../src',              \n",
    "    role=role,\n",
    "    framework_version='1.13.1',        \n",
    "    py_version='py39',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g4dn.xlarge',    \n",
    "    hyperparameters={\n",
    "        'epochs': 100,\n",
    "        'batch-size': 8,\n",
    "        'patience': 5 \n",
    "    },\n",
    "    sagemaker_session=sm_sess\n",
    ")\n",
    "\n",
    "\n",
    "estimator.fit({\n",
    "    'train': f's3://{bucket_name}/data/train',\n",
    "    'test': f's3://{bucket_name}/data/valid'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f50a49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Model di S3: s3://sagemaker-us-east-1-564415061686/pytorch-training-2025-12-30-18-09-16-410/output/model.tar.gz\n",
      "‚¨áÔ∏è Mendownload model...\n",
      "üìÇ Mengekstrak model...\n",
      "‚úÖ Selesai!\n",
      "‚û°Ô∏è Gunakan model: model_hasil_training/model.pth\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# 1. Ambil S3 URI dari estimator\n",
    "model_s3_uri = estimator.model_data\n",
    "print(f\"üì¶ Model di S3: {model_s3_uri}\")\n",
    "\n",
    "# 2. Parsing URI\n",
    "parsed = urlparse(model_s3_uri)\n",
    "bucket = parsed.netloc\n",
    "key = parsed.path.lstrip('/')\n",
    "\n",
    "# 3. Download\n",
    "local_tar = \"model.tar.gz\"\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "print(\"‚¨áÔ∏è Mendownload model...\")\n",
    "s3.download_file(bucket, key, local_tar)\n",
    "\n",
    "# 4. Extract\n",
    "extract_dir = \"model_hasil_training\"\n",
    "os.makedirs(extract_dir, exist_ok=True)\n",
    "\n",
    "print(\"üìÇ Mengekstrak model...\")\n",
    "with tarfile.open(local_tar, \"r:gz\") as tar:\n",
    "    tar.extractall(path=extract_dir)\n",
    "\n",
    "print(\"‚úÖ Selesai!\")\n",
    "print(f\"‚û°Ô∏è Gunakan model: {extract_dir}/model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6b255c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Training ViT-B/16 (SageMaker)\n",
      "‚öôÔ∏è Device: cpu\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './data/train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 210\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Training completed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 210\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 68\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m train_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     53\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((IMG_SIZE, IMG_SIZE)),\n\u001b[0;32m     54\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mRandomHorizontalFlip(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m                          [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m     59\u001b[0m ])\n\u001b[0;32m     61\u001b[0m val_transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     62\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((IMG_SIZE, IMG_SIZE)),\n\u001b[0;32m     63\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     64\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize([\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m],\n\u001b[0;32m     65\u001b[0m                          [\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])\n\u001b[0;32m     66\u001b[0m ])\n\u001b[1;32m---> 68\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(args\u001b[38;5;241m.\u001b[39mtest, transform\u001b[38;5;241m=\u001b[39mval_transform)\n\u001b[0;32m     70\u001b[0m class_names \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mclasses\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\kulit_ai\\lib\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    321\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m ):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\kulit_ai\\lib\\site-packages\\torchvision\\datasets\\folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[0;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\kulit_ai\\lib\\site-packages\\torchvision\\datasets\\folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\kulit_ai\\lib\\site-packages\\torchvision\\datasets\\folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './data/train'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "import copy\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "def train():\n",
    "    print(\"üöÄ Starting Training ViT-B/16 (SageMaker)\")\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=30)\n",
    "    parser.add_argument('--lr', type=float, default=3e-4)\n",
    "    parser.add_argument('--batch-size', type=int, default=16)\n",
    "    parser.add_argument('--patience', type=int, default=5)\n",
    "\n",
    "    parser.add_argument(\n",
    "    '--train',\n",
    "    type=str,\n",
    "    default=os.environ.get('SM_CHANNEL_TRAIN', './data/train')\n",
    ")\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--test',\n",
    "        type=str,\n",
    "        default=os.environ.get('SM_CHANNEL_TEST', './data/valid')\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        '--model-dir',\n",
    "        type=str,\n",
    "        default=os.environ.get('SM_MODEL_DIR', './saved_model')\n",
    "    )\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # safety check\n",
    "    if args.model_dir is None:\n",
    "        args.model_dir = \"./saved_model\"\n",
    "\n",
    "    os.makedirs(args.model_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"‚öôÔ∏è Device: {device}\")\n",
    "\n",
    "    # ---------------- DATA ----------------\n",
    "    IMG_SIZE = 224\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_dataset = datasets.ImageFolder(args.train, transform=train_transform)\n",
    "    val_dataset = datasets.ImageFolder(args.test, transform=val_transform)\n",
    "    class_names = train_dataset.classes\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=args.batch_size,\n",
    "        shuffle=True, num_workers=4\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=args.batch_size,\n",
    "        shuffle=False, num_workers=4\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Classes: {class_names}\")\n",
    "\n",
    "    # ---------------- MODEL ----------------\n",
    "    weights = models.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "    model = models.vit_b_16(weights=weights)\n",
    "\n",
    "    # Freeze backbone\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace head\n",
    "    model.heads.head = nn.Linear(\n",
    "        model.heads.head.in_features,\n",
    "        len(class_names)\n",
    "    )\n",
    "\n",
    "    for param in model.heads.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    # ---------------- TRAIN SETUP ----------------\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        model.heads.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience_counter = 0\n",
    "    best_epoch = 0\n",
    "    best_weights = copy.deepcopy(model.state_dict())\n",
    "    best_preds, best_labels = None, None\n",
    "\n",
    "    # ---------------- TRAIN LOOP ----------------\n",
    "    for epoch in range(args.epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "\n",
    "        train_loss /= len(train_dataset)\n",
    "        train_acc = train_correct / len(train_dataset)\n",
    "\n",
    "        # -------- VALIDATION --------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_dataset)\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "        # -------- PRINT --------\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{args.epochs}] | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # -------- EARLY STOP --------\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            best_preds = all_preds\n",
    "            best_labels = all_labels\n",
    "            patience_counter = 0\n",
    "            print(\"‚úÖ Best model updated\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"‚ö†Ô∏è No improvement ({patience_counter}/{args.patience})\")\n",
    "\n",
    "            if patience_counter >= args.patience:\n",
    "                print(\"üõë Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    # ---------------- REPORT ----------------\n",
    "    print(\"\\nüìä CLASSIFICATION REPORT (BEST EPOCH)\")\n",
    "    print(f\"üèÜ Best Epoch: {best_epoch}\")\n",
    "    print(classification_report(best_labels, best_preds, target_names=class_names))\n",
    "\n",
    "    # ---------------- SAVE (BENAR UNTUK SAGEMAKER) ----------------\n",
    "    save_path = os.path.join(args.model_dir, \"model.pth\")\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": best_weights,\n",
    "            \"class_names\": class_names,\n",
    "            \"img_size\": IMG_SIZE,\n",
    "            \"model_name\": \"vit_b_16\"\n",
    "        },\n",
    "        save_path\n",
    "    )\n",
    "\n",
    "    print(f\"üíæ Model saved to {save_path}\")\n",
    "    print(\"‚úÖ Training completed\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2ca170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kulit_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
